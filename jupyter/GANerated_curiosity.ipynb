{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANerated curiosity",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "SOpg-Q4uO8tM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cMZsJVOCBDLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "btJVe39dBIbS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ]
    },
    {
      "metadata": {
        "id": "P6udten7BGLz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "import numpy as np\n",
        "from pdb import set_trace\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# NN\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions import Categorical\n",
        "import gym\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwoWlqXQBMwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ]
    },
    {
      "metadata": {
        "id": "z7v6VL0M_3XZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BETA = .2\n",
        "LAMBDA = .1\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "LR = 1e-4\n",
        "\n",
        "NUM_EPOCH = 1500\n",
        "NUM_STEP = 3000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SK2Dy4vRxupP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Models"
      ]
    },
    {
      "metadata": {
        "id": "SOpg-Q4uO8tM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ConvBlock"
      ]
    },
    {
      "metadata": {
        "id": "Qr1xypJEx2od",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\" 4 Conv2d + LeakyReLU \"\"\"\n",
        "    def __init__(self, ch_in=1):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        \n",
        "        # constants\n",
        "        self.num_filter = 32\n",
        "        self.size = 3\n",
        "        self.stride = 2\n",
        "        self.pad = self.size//2 \n",
        "\n",
        "        # layers\n",
        "        self.conv1 = nn.Conv2d(ch_in, self.num_filter, self.size, self.stride, self.pad)\n",
        "        self.conv2 = nn.Conv2d(self.num_filter, self.num_filter, self.size, self.stride, self.pad)\n",
        "        self.conv3 = nn.Conv2d(self.num_filter, self.num_filter, self.size, self.stride, self.pad)\n",
        "        self.conv4 = nn.Conv2d(self.num_filter, self.num_filter, self.size, self.stride, self.pad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x))\n",
        "        x = F.leaky_relu(self.conv2(x))\n",
        "        x = F.leaky_relu(self.conv3(x))\n",
        "        x = F.leaky_relu(self.conv4(x))\n",
        "\n",
        "        return torch.flatten(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOKsDyA9PIxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FeatureEncoderNet"
      ]
    },
    {
      "metadata": {
        "id": "oPTq-ghTPGsp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeatureEncoderNet(nn.Module):\n",
        "    \"\"\" Network for feature encoding\n",
        "\n",
        "        In: [s_t]\n",
        "            Current state (i.e. pixels) -> 1 channel image is needed\n",
        "\n",
        "        Out: phi(s_t)\n",
        "            Current state transformed into feature space\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, is_a3c=True):\n",
        "        super(FeatureEncoderNet, self).__init__()\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.h1 = 256\n",
        "        self.is_a3c = is_a3c # indicates whether the LSTM is needed\n",
        "\n",
        "        # layers\n",
        "        self.conv = ConvBlock()\n",
        "        if self.is_a3c:\n",
        "          self.lstm = nn.LSTMCell(input_size=self.in_size, hidden_size=self.h1)\n",
        "\n",
        "          \n",
        "    def reset_lstm(self, x):\n",
        "      if self.is_a3c:\n",
        "          with torch.no_grad():\n",
        "            self.h_t1 = self.c_t1 = torch.zeros(x, self.h1).cuda() if torch.cuda.is_available() else torch.zeros(x,self.h1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        if self.is_a3c:\n",
        "          from pdb import set_trace\n",
        "          #set_trace()\n",
        "          \n",
        "          x = x.view(-1, self.in_size)\n",
        "          \n",
        "          self.h_t1, self.c_t1 = self.lstm(x, (self.h_t1, self.c_t1)) # h_t1 is the output\n",
        "\n",
        "          return self.h_t1#[:, -1, :]#.reshape(-1)\n",
        "        \n",
        "        else:\n",
        "          return x.view(-1, self.in_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKyGMrbwPQS9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## InverseNet"
      ]
    },
    {
      "metadata": {
        "id": "xPTeRbNBPM3n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InverseNet(nn.Module):\n",
        "    \"\"\" Network for the inverse dynamics\n",
        "\n",
        "        In: torch.cat((phi(s_t), phi(s_{t+1}), 1)\n",
        "            Current and next states transformed into the feature space, \n",
        "            denoted by phi().\n",
        "\n",
        "        Out: \\hat{a}_t\n",
        "            Predicted action\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_actions):\n",
        "        super(InverseNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.feat_size = 288\n",
        "        self.fc_hidden = 256\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # layers\n",
        "        #self.conv = ConvBlock()\n",
        "        self.fc1 = nn.Linear(self.feat_size*2, self.fc_hidden)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden, self.num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.fc1(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z7EUOH9mPWnW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ForwardNet"
      ]
    },
    {
      "metadata": {
        "id": "7K4ZnfErPTsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ForwardNet(nn.Module):\n",
        "    \"\"\" Network for the forward dynamics\n",
        "\n",
        "    In: torch.cat((phi(s_t), a_t), 1)\n",
        "        Current state transformed into the feature space, \n",
        "        denoted by phi() and current action\n",
        "\n",
        "    Out: \\hat{phi(s_{t+1})}\n",
        "        Predicted next state (in feature space)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size):\n",
        "\n",
        "        super(ForwardNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.fc_hidden = 256\n",
        "        self.out_size = 288\n",
        "\n",
        "        # layers\n",
        "        self.fc1 = nn.Linear(self.in_size, self.fc_hidden)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden, self.out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #set_trace()\n",
        "        return self.fc2(self.fc1(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cqs2ipzoPadx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## AdversarialHead"
      ]
    },
    {
      "metadata": {
        "id": "WDk2gyiDPZAT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AdversarialHead(nn.Module):\n",
        "    def __init__(self, feat_size, num_actions):\n",
        "        super(AdversarialHead, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.feat_size = feat_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # networks\n",
        "        self.fwd_net = ForwardNet(self.feat_size + self.num_actions)\n",
        "        self.inv_net = InverseNet(num_actions)\n",
        "\n",
        "    def forward(self, phi_t, phi_t1, a_t):\n",
        "        \"\"\"\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "\n",
        "        # forward dynamics\n",
        "        # predict next encoded state\n",
        "        fwd_in = torch.cat((phi_t, a_t), 1) # concatenate next to each other\n",
        "        phi_t1_hat =  self.fwd_net(fwd_in)\n",
        "\n",
        "        # inverse dynamics\n",
        "        # predict the action between s_t and s_t1\n",
        "        inv_in = torch.cat((phi_t, phi_t1), 1)\n",
        "        a_t_hat = self.inv_net(inv_in)\n",
        "\n",
        "\n",
        "        return phi_t1_hat, a_t_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tN3AwHeQPgeq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ICMNet"
      ]
    },
    {
      "metadata": {
        "id": "RA83YkEsPfIi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ICMNet(nn.Module):\n",
        "    def __init__(self, num_actions, in_size=288, feat_size=256):\n",
        "        super(ICMNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size # pixels i.e. state\n",
        "        self.feat_size = feat_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # networks\n",
        "        self.feat_enc_net = FeatureEncoderNet(self.in_size, is_a3c=False)\n",
        "        self.pred_net = AdversarialHead(self.in_size, self.num_actions)     # goal: minimize prediction error \n",
        "        self.policy_net = AdversarialHead(self.in_size, self.num_actions)   # goal: maximize prediction error \n",
        "                                                                            # (i.e. predict states which can contain new information)\n",
        "\n",
        "    def forward(self, s_t, s_t1, a_t):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "            s_t1: next state\n",
        "\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "\n",
        "        # encode the states\n",
        "        \n",
        "        phi_t = self.feat_enc_net(s_t)\n",
        "        phi_t1 = self.feat_enc_net(s_t1)\n",
        "        #set_trace()\n",
        "\n",
        "        # HERE COMES THE NEW THING (currently commented out)\n",
        "        phi_t1_pred, a_t_pred = self.pred_net(phi_t, phi_t1, a_t)\n",
        "        #phi_t1_policy, a_t_policy = self.policy_net_net(phi_t, phi_t1, a_t)\n",
        "\n",
        "\n",
        "        return phi_t1, phi_t1_pred, a_t_pred#(phi_t1_pred, a_t_pred), (phi_t1_policy, a_t_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v91Jxd6vPk6E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A3CNet"
      ]
    },
    {
      "metadata": {
        "id": "sx_XfenJPjtb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class A3CNet(nn.Module):\n",
        "    def __init__(self, num_actions, in_size=288):\n",
        "        super(A3CNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # networks\n",
        "        self.feat_enc_net = FeatureEncoderNet(self.in_size)\n",
        "        self.actor = nn.Linear(self.feat_enc_net.h1, self.num_actions) # estimates what to do\n",
        "        self.critic = nn.Linear(self.feat_enc_net.h1, 1) # estimates how good the value function (how good the current state is)\n",
        "\n",
        "    def forward(self, s_t):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "           \n",
        "            phi_t: current encoded state\n",
        "        \"\"\"\n",
        "        phi_t = self.feat_enc_net(s_t)\n",
        "\n",
        "        policy = self.actor(phi_t)\n",
        "        value = self.critic(phi_t)\n",
        "\n",
        "        return policy, torch.squeeze(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZbgJs5gyBSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ]
    },
    {
      "metadata": {
        "id": "EHOlhMfZyOSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ICMAgent(nn.Module):\n",
        "    def __init__(self, env_name, num_epoch, num_steps,\n",
        "                 discount_factor=DISCOUNT_FACTOR, in_size=288):\n",
        "        super().__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.is_cuda = torch.cuda.is_available()\n",
        "        self.env = gym.make(env_name)\n",
        "        self.num_actions = self.env.action_space.n\n",
        "        self.num_epoch = num_epoch\n",
        "        self.num_steps = num_steps\n",
        "        self.discount_factor = discount_factor\n",
        "        \n",
        "        self.cum_r = 0\n",
        "        \n",
        "        # logging\n",
        "        self.clear_log_lists()\n",
        "       \n",
        "\n",
        "        # networks\n",
        "        self.icm = ICMNet(self.num_actions, self.in_size)\n",
        "        self.a3c = A3CNet(self.num_actions, self.in_size)\n",
        "\n",
        "        if self.is_cuda:\n",
        "            self.icm.cuda()\n",
        "            self.a3c.cuda()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam( list(self.icm.parameters()) + list(self.a3c.parameters()) )\n",
        "        \n",
        "    def clear_log_lists(self):\n",
        "        self.r_l = []\n",
        "        self.s_t_l = []\n",
        "        self.s_t1_l = []\n",
        "        self.a_t_l = []\n",
        "        self.a_t_log_prob_l = []\n",
        "        self.a_t1_l = []\n",
        "        self.policy_l = []\n",
        "        self.value_l = []\n",
        "        \n",
        "                \n",
        "\n",
        "        \n",
        "    def get_action(self, s_t):\n",
        "        # sample actor-critc\n",
        "        policy, value = self.a3c(s_t) # use A3C to get policy and value\n",
        "        \n",
        "        # determine action\n",
        "        action_prob = F.softmax(policy, dim=-1)\n",
        "        cat = Categorical(action_prob)\n",
        "        a_t = cat.sample() # detach for action?\n",
        "        \n",
        "        # append current action, policy and value\n",
        "        self.a_t_l.append(a_t)\n",
        "        self.a_t_log_prob_l.append(cat.log_prob(a_t))\n",
        "        self.policy_l.append(policy)\n",
        "        self.value_l.append(value)\n",
        "        \n",
        "\n",
        "        from pdb import set_trace\n",
        "        #set_trace()\n",
        "        return a_t#, value, policy\n",
        "\n",
        "\n",
        "    \n",
        "    def pix2tensor(self, pix):\n",
        "        im2tensor = transforms.Compose([transforms.ToPILImage(),\n",
        "                                        transforms.Grayscale(1),\n",
        "                                        transforms.Resize((42,42)),\n",
        "                                        transforms.ToTensor()])\n",
        "\n",
        "        return im2tensor(pix).cuda()\n",
        "      \n",
        "    def play(self):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "            s_t1: next state\n",
        "\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "                \n",
        "        # reset all logger lists\n",
        "        self.clear_log_lists()\n",
        "        \n",
        "        self.a3c.feat_enc_net.reset_lstm(1)\n",
        "\n",
        "        # play one game\n",
        "        s_t  = self.env.reset()\n",
        "        self.s_t_l.append(self.pix2tensor(s_t)) # append current state\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            a_t = self.get_action(torch.unsqueeze(self.s_t_l[-1],0)) # select action from the policy\n",
        "            \n",
        "            # interact with the environment\n",
        "            s_t, r, done, info = self.env.step(a_t)\n",
        "            s_t = self.pix2tensor(s_t)\n",
        "\n",
        "            # append next state and reward\n",
        "            self.s_t_l.append(s_t)\n",
        "            self.r_l.append(r)\n",
        "            \n",
        "            if done:\n",
        "              print(\"Episode finished\")\n",
        "              break\n",
        "\n",
        "    def normalize(self, data):\n",
        "      return (data - data.mean()) / (data.std() + 10e-9)\n",
        "    def reward_update(self):\n",
        "      policy_loss = []\n",
        "      value_loss = []\n",
        "      r_disc = [0] * len(self.r_l)\n",
        "      rev_idx = list(range(len(self.r_l))) \n",
        "      \n",
        "      \"\"\"Calculate discounted rewards\"\"\"\n",
        "      R = 0 # cumulated reward\n",
        "      for r, r_idx in zip(self.r_l, rev_idx):\n",
        "        R = r + self.discount_factor * R\n",
        "        r_disc[r_idx] = R\n",
        "        \n",
        "      r_disc = torch.tensor(r_disc).cuda() # tensorize\n",
        "      r_disc = self.normalize(r_disc)\n",
        "      \n",
        "      return r_disc\n",
        "      \n",
        "      \n",
        "    def a3c_loss(self):\n",
        "      \"\"\"returns: discounted values\"\"\"\n",
        "      \n",
        "      returns = self.reward_update()\n",
        "      \n",
        "      policy_losses = []\n",
        "      value_losses = []\n",
        "      \n",
        "      # grab the log probability of the action taken, the value associated to it\n",
        "      # and the reward\n",
        "      for log_prob, value, R in zip(self.a_t_log_prob_l, self.value_l, returns):\n",
        "        \n",
        "        # calculate advantage\n",
        "        # i.e. how good was the estimate of the value of the current state\n",
        "        advantage = R - value.item()\n",
        "        \n",
        "        # weight the deviation of the predicted value (of the state) from the \n",
        "        # actual reward (=advantage) with the negative log probability of the action\n",
        "        # taken (- needed as log(p) p in [0;1] < 0)\n",
        "        policy_losses.append(-log_prob * advantage)\n",
        "        \n",
        "        # the value loss weights the squared difference between the actual\n",
        "        # and predicted rewards\n",
        "        value_losses.append(F.mse_loss(value, torch.tensor([R]).cuda()))\n",
        "        \n",
        "      \n",
        "      # return the a3c loss\n",
        "      # which is the sum of the actor (policy) and critic (advantage) losses\n",
        "      loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "      \n",
        "      return loss\n",
        "        \n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "            s_t1: next state\n",
        "\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "        for epoch in range(self.num_epoch):\n",
        "          \n",
        "          # play an episode\n",
        "          self.play()\n",
        "          \n",
        "          s_t_stacked = self.s_t_l\n",
        "          #s_t_stacked = [self.s_t_l[0]]*3 +  self.s_t_l\n",
        "          #set_trace()\n",
        "          #s_t_stacked = [torch.stack(s_t_stacked[i:i+3], 3) for i in range(0,len(s_t_stacked)-3)]\n",
        "          #s_t_stacked = [i.view(i.shape[0], i.shape[1],-1) for i in s_t_stacked]\n",
        "          #set_trace()\n",
        "          \n",
        "          s_t = torch.stack(s_t_stacked[0:-1]) # last value is not needed here\n",
        "          s_t1 = torch.stack(s_t_stacked[1:])\n",
        "          \n",
        "          a_t = torch.FloatTensor(self.a_t_l)\n",
        "          # convert the action tensor into one-hot\n",
        "          a_t_1_hot = torch.zeros(a_t.shape[0],self.num_actions).scatter_(1, a_t.long().view(-1,1),1)\n",
        "          \n",
        "          if self.is_cuda:\n",
        "            s_t = s_t.cuda()\n",
        "            s_t1 = s_t1.cuda()\n",
        "            a_t = a_t.cuda()\n",
        "            a_t_1_hot = a_t_1_hot.cuda()\n",
        "            \n",
        "          \n",
        "          # reset LSTM hidden states\n",
        "          #self.a3c.feat_enc_net.reset_lstm(s_t.shape[0])  # specify size \n",
        "          \n",
        "\n",
        "          # call the ICM model         \n",
        "          self.icm.feat_enc_net.reset_lstm(s_t.shape[0])\n",
        "          phi_t1, phi_t1_pred, a_t_pred = self.icm(s_t, s_t1, a_t_1_hot)\n",
        "\n",
        "\n",
        "          \"\"\"calculate losses\"\"\"\n",
        "          self.optimizer.zero_grad()\n",
        "          \n",
        "          # forward loss\n",
        "          # discrepancy between the predicted and actual next states\n",
        "          loss_forward = F.mse_loss(phi_t1_pred, phi_t1)\n",
        "          \n",
        "          # inverse loss\n",
        "          # cross entropy between the predicted and actual actions\n",
        "          loss_inv = F.cross_entropy(a_t_pred, a_t.long().view(-1))\n",
        "          \n",
        "          # a3c loss\n",
        "          # loss of the policy (how good can we choose the proper action)\n",
        "          # and the advantage function (how good is the estimate of the value \n",
        "          # of the current state)\n",
        "          loss_a3c = self.a3c_loss()\n",
        "\n",
        "          \n",
        "          # compose losses\n",
        "          loss = BETA * loss_forward + (1-BETA) * loss_inv + LAMBDA * loss_a3c\n",
        "\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          \n",
        "          print(\"Epoch: {}, loss {}\".format(epoch, loss) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qjflexYKyT_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "id": "01aK3DKAyO0O",
        "colab_type": "code",
        "outputId": "bf8e7088-5000-46cf-ee62-5599bcd6e2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# objects\n",
        "#env = gym.make('MsPacman-v0')\n",
        "#env = gym.make('MontezumaRevenge-v0')\n",
        "agent = ICMAgent('Pong-v0', NUM_EPOCH, NUM_STEP)\n",
        "\n",
        "agent.cuda()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ICMAgent(\n",
              "  (icm): ICMNet(\n",
              "    (feat_enc_net): FeatureEncoderNet(\n",
              "      (conv): ConvBlock(\n",
              "        (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (pred_net): AdversarialHead(\n",
              "      (fwd_net): ForwardNet(\n",
              "        (fc1): Linear(in_features=294, out_features=256, bias=True)\n",
              "        (fc2): Linear(in_features=256, out_features=288, bias=True)\n",
              "      )\n",
              "      (inv_net): InverseNet(\n",
              "        (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
              "        (fc2): Linear(in_features=256, out_features=6, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (policy_net): AdversarialHead(\n",
              "      (fwd_net): ForwardNet(\n",
              "        (fc1): Linear(in_features=294, out_features=256, bias=True)\n",
              "        (fc2): Linear(in_features=256, out_features=288, bias=True)\n",
              "      )\n",
              "      (inv_net): InverseNet(\n",
              "        (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
              "        (fc2): Linear(in_features=256, out_features=6, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (a3c): A3CNet(\n",
              "    (feat_enc_net): FeatureEncoderNet(\n",
              "      (conv): ConvBlock(\n",
              "        (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      )\n",
              "      (lstm): LSTMCell(288, 256)\n",
              "    )\n",
              "    (actor): Linear(in_features=256, out_features=6, bias=True)\n",
              "    (critic): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "jqaMdeS95Qby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PE90R77hCURZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXM65D7zIIX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "agent.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RXKlSMTKwpo3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "agent.num_epoch = 200\n",
        "agent.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLqWqCWcAO0c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "s_t = agent.env.reset()\n",
        "plt.figure(figsize=(9,9))\n",
        "\n",
        "for _ in range(2000):\n",
        "    plt.imshow(agent.env.render(mode='rgb_array')) # just update the data\n",
        "    #display.display(plt.gcf())    \n",
        "    display.clear_output(wait=True)\n",
        "    agent.get_action(torch.unsqueeze(agent.pix2tensor(s_t),0))\n",
        "    s_t, r, done, info = agent.env.step(agent.a_t_l[-1])\n",
        "agent.env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sFo2ZRfFI6i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(agent.state_dict(), \"agent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZIbmGR6EIaib",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "aa = torch.load(\"agent\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gToNTefwIpJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a =  ICMAgent('Pong-v0', NUM_EPOCH, NUM_STEP)\n",
        "a.load_state_dict(aa)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtbUsUzgIfak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "s_t = a.env.reset()\n",
        "plt.figure(figsize=(9,9))\n",
        "\n",
        "for _ in range(2000):\n",
        "    plt.imshow(a.env.render(mode='rgb_array')) # just update the data\n",
        "    #display.display(plt.gcf())    \n",
        "    display.clear_output(wait=True)\n",
        "    a.get_action(torch.unsqueeze(a.pix2tensor(s_t),0))\n",
        "    s_t, r, done, info = a.env.step(a.a_t_l[-1])\n",
        "a.env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}