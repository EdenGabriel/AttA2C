{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANerated curiosity",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cMZsJVOCBDLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "btJVe39dBIbS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ]
    },
    {
      "metadata": {
        "id": "P6udten7BGLz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "import numpy as np\n",
        "from pdb import set_trace\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# NN\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions import Categorical\n",
        "import gym\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwoWlqXQBMwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ]
    },
    {
      "metadata": {
        "id": "z7v6VL0M_3XZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BETA = .2\n",
        "LAMBDA = .1\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "LR = 1e-3\n",
        "\n",
        "NUM_EPOCH = 200\n",
        "NUM_STEP = 3000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SK2Dy4vRxupP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Models"
      ]
    },
    {
      "metadata": {
        "id": "SOpg-Q4uO8tM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ConvBlock"
      ]
    },
    {
      "metadata": {
        "id": "Qr1xypJEx2od",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\" 4 Conv2d + LeakyReLU \"\"\"\n",
        "    def __init__(self, ch_in=1):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        \n",
        "        # constants\n",
        "        self.num_filter = 32\n",
        "        self.size = 3\n",
        "        self.stride = 2\n",
        "        self.pad = self.size//2 \n",
        "\n",
        "        # layers\n",
        "        self.conv1 = nn.Conv2d(ch_in, self.num_filter, self.size, self.stride, self.pad)\n",
        "        self.conv2 = nn.Conv2d(self.num_filter, self.num_filter, self.size, self.stride, self.pad)\n",
        "        self.conv3 = nn.Conv2d(self.num_filter, self.num_filter, self.size, self.stride, self.pad)\n",
        "        self.conv4 = nn.Conv2d(self.num_filter, self.num_filter, self.size, self.stride, self.pad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x))\n",
        "        x = F.leaky_relu(self.conv2(x))\n",
        "        x = F.leaky_relu(self.conv3(x))\n",
        "        x = F.leaky_relu(self.conv4(x))\n",
        "\n",
        "        return torch.flatten(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOKsDyA9PIxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FeatureEncoderNet"
      ]
    },
    {
      "metadata": {
        "id": "oPTq-ghTPGsp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeatureEncoderNet(nn.Module):\n",
        "    \"\"\" Network for feature encoding\n",
        "\n",
        "        In: [s_t]\n",
        "            Current state (i.e. pixels) -> 1 channel image is needed\n",
        "\n",
        "        Out: phi(s_t)\n",
        "            Current state transformed into feature space\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, is_a3c=True):\n",
        "        super(FeatureEncoderNet, self).__init__()\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.h1 = 256\n",
        "        self.is_a3c = is_a3c # indicates whether the LSTM is needed\n",
        "\n",
        "        # layers\n",
        "        self.conv = ConvBlock()\n",
        "        if self.is_a3c:\n",
        "          self.lstm = nn.LSTMCell(input_size=self.in_size, hidden_size=self.h1)\n",
        "\n",
        "          \n",
        "    def reset_lstm(self, x):\n",
        "      if self.is_a3c:\n",
        "          with torch.no_grad():\n",
        "            self.h_t1 = self.c_t1 = torch.zeros(x, self.h1).cuda() if torch.cuda.is_available() else torch.zeros(x,self.h1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        if self.is_a3c:\n",
        "          from pdb import set_trace\n",
        "          #set_trace()\n",
        "          \n",
        "          x = x.view(-1, self.in_size)\n",
        "          \n",
        "          self.h_t1, self.c_t1 = self.lstm(x, (self.h_t1, self.c_t1)) # h_t1 is the output\n",
        "\n",
        "          return self.h_t1#[:, -1, :]#.reshape(-1)\n",
        "        \n",
        "        else:\n",
        "          return x.view(-1, self.in_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKyGMrbwPQS9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## InverseNet"
      ]
    },
    {
      "metadata": {
        "id": "xPTeRbNBPM3n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InverseNet(nn.Module):\n",
        "    \"\"\" Network for the inverse dynamics\n",
        "\n",
        "        In: torch.cat((phi(s_t), phi(s_{t+1}), 1)\n",
        "            Current and next states transformed into the feature space, \n",
        "            denoted by phi().\n",
        "\n",
        "        Out: \\hat{a}_t\n",
        "            Predicted action\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_actions):\n",
        "        super(InverseNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        #self.conv_out = 288\n",
        "        self.feat_size = 288\n",
        "        self.fc_hidden = 256\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # layers\n",
        "        #self.conv = ConvBlock()\n",
        "        self.fc1 = nn.Linear(self.feat_size*2, self.fc_hidden)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden, self.num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.fc1(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z7EUOH9mPWnW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ForwardNet"
      ]
    },
    {
      "metadata": {
        "id": "7K4ZnfErPTsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ForwardNet(nn.Module):\n",
        "    \"\"\" Network for the forward dynamics\n",
        "\n",
        "    In: torch.cat((phi(s_t), a_t), 1)\n",
        "        Current state transformed into the feature space, \n",
        "        denoted by phi() and current action\n",
        "\n",
        "    Out: \\hat{phi(s_{t+1})}\n",
        "        Predicted next state (in feature space)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size):\n",
        "\n",
        "        super(ForwardNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.fc_hidden = 256\n",
        "        self.out_size = 288\n",
        "\n",
        "        # layers\n",
        "        self.fc1 = nn.Linear(self.in_size, self.fc_hidden)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden, self.out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #set_trace()\n",
        "        return self.fc2(self.fc1(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cqs2ipzoPadx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## AdversarialHead"
      ]
    },
    {
      "metadata": {
        "id": "WDk2gyiDPZAT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AdversarialHead(nn.Module):\n",
        "    def __init__(self, feat_size, num_actions):\n",
        "        super(AdversarialHead, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.feat_size = feat_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # networks\n",
        "        self.fwd_net = ForwardNet(self.feat_size + self.num_actions)\n",
        "        self.inv_net = InverseNet(num_actions)\n",
        "\n",
        "    def forward(self, phi_t, phi_t1, a_t):\n",
        "        \"\"\"\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "\n",
        "        # forward dynamics\n",
        "        # predict next encoded state\n",
        "        fwd_in = torch.cat((phi_t, a_t), 1) # concatenate next to each other\n",
        "        phi_t1_hat =  self.fwd_net(fwd_in)\n",
        "\n",
        "        # inverse dynamics\n",
        "        # predict the action between s_t and s_t1\n",
        "        inv_in = torch.cat((phi_t, phi_t1), 1)\n",
        "        a_t_hat = self.inv_net(inv_in)\n",
        "\n",
        "\n",
        "        return phi_t1_hat, a_t_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tN3AwHeQPgeq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ICMNet"
      ]
    },
    {
      "metadata": {
        "id": "RA83YkEsPfIi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ICMNet(nn.Module):\n",
        "    def __init__(self, num_actions, in_size=288, feat_size=256):\n",
        "        super(ICMNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size # pixels i.e. state\n",
        "        self.feat_size = feat_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # networks\n",
        "        self.feat_enc_net = FeatureEncoderNet(self.in_size, is_a3c=False)\n",
        "        self.pred_net = AdversarialHead(self.in_size, self.num_actions)     # goal: minimize prediction error \n",
        "        self.policy_net = AdversarialHead(self.in_size, self.num_actions)   # goal: maximize prediction error \n",
        "                                                                            # (i.e. predict states which can contain new information)\n",
        "\n",
        "    def forward(self, s_t, s_t1, a_t):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "            s_t1: next state\n",
        "\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "\n",
        "        # encode the states\n",
        "        \n",
        "        phi_t = self.feat_enc_net(s_t)\n",
        "        phi_t1 = self.feat_enc_net(s_t1)\n",
        "        #set_trace()\n",
        "\n",
        "        # HERE COMES THE NEW THING (currently commented out)\n",
        "        phi_t1_pred, a_t_pred = self.pred_net(phi_t, phi_t1, a_t)\n",
        "        #phi_t1_policy, a_t_policy = self.policy_net_net(phi_t, phi_t1, a_t)\n",
        "\n",
        "\n",
        "        return phi_t1, phi_t1_pred, a_t_pred#(phi_t1_pred, a_t_pred), (phi_t1_policy, a_t_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v91Jxd6vPk6E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A3CNet"
      ]
    },
    {
      "metadata": {
        "id": "sx_XfenJPjtb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class A3CNet(nn.Module):\n",
        "    def __init__(self, num_actions, in_size=288):\n",
        "        super(A3CNet, self).__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # networks\n",
        "        self.feat_enc_net = FeatureEncoderNet(self.in_size)\n",
        "        self.actor = nn.Linear(self.feat_enc_net.h1, self.num_actions) # estimates what to do\n",
        "        self.critic = nn.Linear(self.feat_enc_net.h1, 1) # estimates how good the value function (how good the current state is)\n",
        "\n",
        "    def forward(self, s_t):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "           \n",
        "            phi_t: current encoded state\n",
        "        \"\"\"\n",
        "        phi_t = self.feat_enc_net(s_t)\n",
        "\n",
        "        policy = self.actor(phi_t)\n",
        "        value = self.critic(phi_t)\n",
        "\n",
        "        return policy, torch.squeeze(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZbgJs5gyBSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ]
    },
    {
      "metadata": {
        "id": "EHOlhMfZyOSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ICMAgent(nn.Module):\n",
        "    def __init__(self, env_name, num_epoch, num_steps,\n",
        "                 discount_factor=DISCOUNT_FACTOR, in_size=288):\n",
        "        super().__init__()\n",
        "\n",
        "        # constants\n",
        "        self.in_size = in_size\n",
        "        self.is_cuda = torch.cuda.is_available()\n",
        "        self.env = gym.make(env_name)\n",
        "        self.num_actions = self.env.action_space.n\n",
        "        self.num_epoch = num_epoch\n",
        "        self.num_steps = num_steps\n",
        "        self.discount_factor = discount_factor\n",
        "        \n",
        "        self.cum_r = 0\n",
        "        \n",
        "        # logging\n",
        "        self.clear_log_lists()\n",
        "       \n",
        "\n",
        "        # networks\n",
        "        self.icm = ICMNet(self.num_actions, self.in_size)\n",
        "        self.a3c = A3CNet(self.num_actions, self.in_size)\n",
        "\n",
        "        if self.is_cuda:\n",
        "            self.icm.cuda()\n",
        "            self.a3c.cuda()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam( list(self.icm.parameters()) + list(self.a3c.parameters()) )\n",
        "        \n",
        "    def clear_log_lists(self):\n",
        "        self.r_l = []\n",
        "        self.s_t_l = []\n",
        "        self.s_t1_l = []\n",
        "        self.a_t_l = []\n",
        "        self.a_t_log_prob_l = []\n",
        "        self.a_t1_l = []\n",
        "        self.policy_l = []\n",
        "        self.value_l = []\n",
        "        \n",
        "                \n",
        "\n",
        "        \n",
        "    def get_action(self, s_t):\n",
        "        policy, value = self.a3c(s_t) # use A3C to get policy and value\n",
        "        action_prob = F.softmax(policy, dim=-1)#.data.cpu().numpy()\n",
        "        #action_prob = action_prob[0,:,:] #remove first dimension\n",
        "        cat = Categorical(action_prob)\n",
        "        a_t = cat.sample() # detach for action?\n",
        "        \n",
        "        # append current action, policy and value\n",
        "        self.a_t_l.append(a_t)\n",
        "        self.a_t_log_prob_l.append(cat.log_prob(a_t))\n",
        "        self.policy_l.append(policy)\n",
        "        self.value_l.append(value)\n",
        "        \n",
        "\n",
        "        from pdb import set_trace\n",
        "        #set_trace()\n",
        "        return a_t#, value, policy#.detach()\n",
        "\n",
        "\n",
        "    # functions\n",
        "    def pix2tensor(self, pix):\n",
        "        im2tensor = transforms.Compose([transforms.ToPILImage(),\n",
        "                                        transforms.Grayscale(1),\n",
        "                                        transforms.Resize((42,42)),\n",
        "                                        transforms.ToTensor()])\n",
        "\n",
        "        return im2tensor(pix).cuda()\n",
        "      \n",
        "    def play(self):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "            s_t1: next state\n",
        "\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "                \n",
        "        # reset all logger lists\n",
        "        self.clear_log_lists()\n",
        "        \n",
        "        self.a3c.feat_enc_net.reset_lstm(1)\n",
        "\n",
        "        # play one game\n",
        "        s_t  = self.env.reset()\n",
        "        self.s_t_l.append(self.pix2tensor(s_t)) # append current state\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            a_t = self.get_action(torch.unsqueeze(self.s_t_l[-1],0)) # select action from the policy\n",
        "            \n",
        "            # interact with the environment\n",
        "            s_t, r, done, info = self.env.step(a_t)\n",
        "            s_t = self.pix2tensor(s_t)\n",
        "\n",
        "            # append next state and reward\n",
        "            self.s_t_l.append(s_t)\n",
        "            self.r_l.append(r)\n",
        "            \n",
        "            if done:\n",
        "              print(\"Episode finished\")\n",
        "              break\n",
        "\n",
        "    def reward_update(self):\n",
        "      policy_loss = []\n",
        "      value_loss = []\n",
        "      r_disc = []\n",
        "      \n",
        "      \"\"\"Calculate discounted rewards\"\"\"\n",
        "      R = 0 # cumulated reward\n",
        "      for r in self.r_l:\n",
        "        R = r + self.discount_factor * R\n",
        "        r_disc.insert(0,R)\n",
        "        \n",
        "      r_disc = torch.tensor(r_disc).cuda() # tensorize\n",
        "      r_disc = (r_disc - r_disc.mean()) / (r_disc.std() + 10e-9) # normalize\n",
        "      \n",
        "      return r_disc\n",
        "      \n",
        "      \n",
        "    def a3c_loss(self):\n",
        "      \"\"\"returns: discounted values\"\"\"\n",
        "      \n",
        "      returns = self.reward_update()\n",
        "      \n",
        "      policy_losses = []\n",
        "      value_losses = []\n",
        "      \n",
        "      # grab the log probability of the action taken, the value associated to it\n",
        "      # and the reward\n",
        "      for log_prob, value, R in zip(self.a_t_log_prob_l, self.value_l, returns):\n",
        "        \n",
        "        # calculate advantage\n",
        "        # i.e. how good was the estimate of the value of the current state\n",
        "        advantage = R - value.item()\n",
        "        \n",
        "        # weight the deviation of the predicted value (of the state) from the \n",
        "        # actual reward (=advantage) with the negative log probability of the action\n",
        "        # taken (- needed as log(p) p in [0;1] < 0)\n",
        "        policy_losses.append(-log_prob * advantage)\n",
        "        \n",
        "        # the value loss weights the squared difference between the actual\n",
        "        # and predicted rewards\n",
        "        value_losses.append(F.mse_loss(value, torch.tensor([R]).cuda()))\n",
        "        \n",
        "      \n",
        "      # return the a3c loss\n",
        "      # which is the sum of the actor (policy) and critic (advantage) losses\n",
        "      \n",
        "      loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "      \n",
        "      return loss\n",
        "        \n",
        "        \n",
        "      \n",
        "             \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "            s_t : current state\n",
        "            s_t1: next state\n",
        "\n",
        "            phi_t: current encoded state\n",
        "            phi_t1: next encoded state\n",
        "\n",
        "            a_t: current action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "        for epoch in range(self.num_epoch):\n",
        "          \n",
        "          # play an episode\n",
        "          self.play()\n",
        "          \n",
        "          s_t_stacked = self.s_t_l\n",
        "          #s_t_stacked = [self.s_t_l[0]]*3 +  self.s_t_l\n",
        "          #set_trace()\n",
        "          #s_t_stacked = [torch.stack(s_t_stacked[i:i+3], 3) for i in range(0,len(s_t_stacked)-3)]\n",
        "          #s_t_stacked = [i.view(i.shape[0], i.shape[1],-1) for i in s_t_stacked]\n",
        "          #set_trace()\n",
        "          \n",
        "          s_t = torch.stack(s_t_stacked[0:-1]) # last value is not needed here\n",
        "          s_t1 = torch.stack(s_t_stacked[1:])\n",
        "          \n",
        "\n",
        "\n",
        "          a_t = torch.FloatTensor(self.a_t_l)\n",
        "          # convert the action tensor into one-hot\n",
        "          a_t_1_hot = torch.zeros(a_t.shape[0],self.num_actions).scatter_(1, a_t.long().view(-1,1),1)\n",
        "          \n",
        "          if self.is_cuda:\n",
        "            s_t = s_t.cuda()\n",
        "            s_t1 = s_t1.cuda()\n",
        "            a_t = a_t.cuda()\n",
        "            a_t_1_hot = a_t_1_hot.cuda()\n",
        "            \n",
        "          \n",
        "          # reset LSTM hidden states\n",
        "          #self.a3c.feat_enc_net.reset_lstm(s_t.shape[0])  # specify size \n",
        "          \n",
        "          \n",
        "          from pdb import set_trace\n",
        "          #set_trace()\n",
        "\n",
        "          # call the ICM model         \n",
        "          self.icm.feat_enc_net.reset_lstm(s_t.shape[0])\n",
        "          phi_t1, phi_t1_pred, a_t_pred = self.icm(s_t, s_t1, a_t_1_hot)\n",
        "\n",
        "\n",
        "          \"\"\"calculate losses\"\"\"\n",
        "          self.optimizer.zero_grad()\n",
        "          \n",
        "          # forward loss\n",
        "          # discrepancy between the predicted and actual next states\n",
        "          loss_forward = F.mse_loss(phi_t1_pred, phi_t1)\n",
        "          \n",
        "          # inverse loss\n",
        "          # cross entropy between the predicted and actual actions\n",
        "          loss_inv = F.cross_entropy(a_t_pred, a_t.long().view(-1))\n",
        "          \n",
        "          # a3c loss\n",
        "          # loss of the policy (how good can we choose the proper action)\n",
        "          # and the advantage function (how good is the estimate of the value \n",
        "          # of the current state)\n",
        "          loss_a3c = self.a3c_loss()\n",
        "\n",
        "          \n",
        "          # compose losses\n",
        "          loss = BETA * loss_forward + (1-BETA) * loss_inv + LAMBDA * loss_a3c\n",
        "\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          \n",
        "          print(\"Epoch: {}, loss {}\".format(epoch, loss) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qjflexYKyT_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "id": "01aK3DKAyO0O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# objects\n",
        "#env = gym.make('MsPacman-v0')\n",
        "#env = gym.make('MontezumaRevenge-v0')\n",
        "agent = ICMAgent('Pong-v0', NUM_EPOCH, NUM_STEP)\n",
        "\n",
        "agent.cuda()\n",
        "agent.train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqaMdeS95Qby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PE90R77hCURZ",
        "colab_type": "code",
        "outputId": "aa30b795-0c74-4729-93d0-bb1c85b714a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6817
        }
      },
      "cell_type": "code",
      "source": [
        "agent.train()"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode finished\n",
            "Epoch: 0, loss 61.54340362548828\n",
            "Episode finished\n",
            "Epoch: 1, loss 58.91178512573242\n",
            "Episode finished\n",
            "Epoch: 2, loss 33.65998077392578\n",
            "Episode finished\n",
            "Epoch: 3, loss 63.651248931884766\n",
            "Episode finished\n",
            "Epoch: 4, loss 133.27854919433594\n",
            "Episode finished\n",
            "Epoch: 5, loss 31.519001007080078\n",
            "Episode finished\n",
            "Epoch: 6, loss 164.44361877441406\n",
            "Episode finished\n",
            "Epoch: 7, loss 129.20599365234375\n",
            "Episode finished\n",
            "Epoch: 8, loss 192.78878784179688\n",
            "Episode finished\n",
            "Epoch: 9, loss 161.34274291992188\n",
            "Episode finished\n",
            "Epoch: 10, loss 101.743408203125\n",
            "Episode finished\n",
            "Epoch: 11, loss 136.18333435058594\n",
            "Episode finished\n",
            "Epoch: 12, loss 87.29539489746094\n",
            "Episode finished\n",
            "Epoch: 13, loss 113.50727081298828\n",
            "Episode finished\n",
            "Epoch: 14, loss 96.94348907470703\n",
            "Episode finished\n",
            "Epoch: 15, loss 112.46073150634766\n",
            "Episode finished\n",
            "Epoch: 16, loss 43.080101013183594\n",
            "Episode finished\n",
            "Epoch: 17, loss 105.36235046386719\n",
            "Episode finished\n",
            "Epoch: 18, loss 91.56072235107422\n",
            "Episode finished\n",
            "Epoch: 19, loss 47.53602981567383\n",
            "Episode finished\n",
            "Epoch: 20, loss 75.46094512939453\n",
            "Episode finished\n",
            "Epoch: 21, loss 59.172149658203125\n",
            "Episode finished\n",
            "Epoch: 22, loss 43.079315185546875\n",
            "Episode finished\n",
            "Epoch: 23, loss 49.38376235961914\n",
            "Episode finished\n",
            "Epoch: 24, loss 33.41433334350586\n",
            "Episode finished\n",
            "Epoch: 25, loss 39.643150329589844\n",
            "Episode finished\n",
            "Epoch: 26, loss 44.87626266479492\n",
            "Episode finished\n",
            "Epoch: 27, loss 101.30341339111328\n",
            "Episode finished\n",
            "Epoch: 28, loss 117.4977798461914\n",
            "Episode finished\n",
            "Epoch: 29, loss 106.9726791381836\n",
            "Episode finished\n",
            "Epoch: 30, loss 99.18822479248047\n",
            "Episode finished\n",
            "Epoch: 31, loss 84.56173706054688\n",
            "Episode finished\n",
            "Epoch: 32, loss 84.01252746582031\n",
            "Episode finished\n",
            "Epoch: 33, loss 82.97113800048828\n",
            "Episode finished\n",
            "Epoch: 34, loss 51.441463470458984\n",
            "Episode finished\n",
            "Epoch: 35, loss 42.38834762573242\n",
            "Episode finished\n",
            "Epoch: 36, loss 23.661518096923828\n",
            "Episode finished\n",
            "Epoch: 37, loss 105.71166229248047\n",
            "Episode finished\n",
            "Epoch: 38, loss 26.902549743652344\n",
            "Episode finished\n",
            "Epoch: 39, loss 112.9984359741211\n",
            "Episode finished\n",
            "Epoch: 40, loss 115.12491607666016\n",
            "Episode finished\n",
            "Epoch: 41, loss 80.31233215332031\n",
            "Episode finished\n",
            "Epoch: 42, loss 159.8999481201172\n",
            "Episode finished\n",
            "Epoch: 43, loss 120.10317993164062\n",
            "Episode finished\n",
            "Epoch: 44, loss 130.75718688964844\n",
            "Episode finished\n",
            "Epoch: 45, loss 131.48098754882812\n",
            "Episode finished\n",
            "Epoch: 46, loss 73.50901794433594\n",
            "Episode finished\n",
            "Epoch: 47, loss 105.86064910888672\n",
            "Episode finished\n",
            "Epoch: 48, loss 95.35517120361328\n",
            "Episode finished\n",
            "Epoch: 49, loss 116.96208953857422\n",
            "Episode finished\n",
            "Epoch: 50, loss 59.81264114379883\n",
            "Episode finished\n",
            "Epoch: 51, loss 42.23885726928711\n",
            "Episode finished\n",
            "Epoch: 52, loss 64.13261413574219\n",
            "Episode finished\n",
            "Epoch: 53, loss 47.550086975097656\n",
            "Episode finished\n",
            "Epoch: 54, loss 75.65935516357422\n",
            "Episode finished\n",
            "Epoch: 55, loss 87.4305419921875\n",
            "Episode finished\n",
            "Epoch: 56, loss 94.5737075805664\n",
            "Episode finished\n",
            "Epoch: 57, loss 78.53834533691406\n",
            "Episode finished\n",
            "Epoch: 58, loss 95.46748352050781\n",
            "Episode finished\n",
            "Epoch: 59, loss 137.74847412109375\n",
            "Episode finished\n",
            "Epoch: 60, loss 119.49537658691406\n",
            "Episode finished\n",
            "Epoch: 61, loss 157.65797424316406\n",
            "Episode finished\n",
            "Epoch: 62, loss 125.64469146728516\n",
            "Episode finished\n",
            "Epoch: 63, loss 156.91177368164062\n",
            "Episode finished\n",
            "Epoch: 64, loss 133.26504516601562\n",
            "Episode finished\n",
            "Epoch: 65, loss 89.1257553100586\n",
            "Episode finished\n",
            "Epoch: 66, loss 158.81100463867188\n",
            "Episode finished\n",
            "Epoch: 67, loss 101.21315002441406\n",
            "Episode finished\n",
            "Epoch: 68, loss 93.17987823486328\n",
            "Episode finished\n",
            "Epoch: 69, loss 75.08861541748047\n",
            "Episode finished\n",
            "Epoch: 70, loss 100.93514251708984\n",
            "Episode finished\n",
            "Epoch: 71, loss 80.5364990234375\n",
            "Episode finished\n",
            "Epoch: 72, loss 109.55398559570312\n",
            "Episode finished\n",
            "Epoch: 73, loss 31.02498435974121\n",
            "Episode finished\n",
            "Epoch: 74, loss 63.84769821166992\n",
            "Episode finished\n",
            "Epoch: 75, loss 46.0907096862793\n",
            "Episode finished\n",
            "Epoch: 76, loss 50.86176681518555\n",
            "Episode finished\n",
            "Epoch: 77, loss 68.82652282714844\n",
            "Episode finished\n",
            "Epoch: 78, loss 86.41510772705078\n",
            "Episode finished\n",
            "Epoch: 79, loss 36.66837692260742\n",
            "Episode finished\n",
            "Epoch: 80, loss 156.5741424560547\n",
            "Episode finished\n",
            "Epoch: 81, loss 128.68995666503906\n",
            "Episode finished\n",
            "Epoch: 82, loss 108.58643341064453\n",
            "Episode finished\n",
            "Epoch: 83, loss 144.5181427001953\n",
            "Episode finished\n",
            "Epoch: 84, loss 91.47659301757812\n",
            "Episode finished\n",
            "Epoch: 85, loss 57.01457977294922\n",
            "Episode finished\n",
            "Epoch: 86, loss 113.9713363647461\n",
            "Episode finished\n",
            "Epoch: 87, loss 165.17007446289062\n",
            "Episode finished\n",
            "Epoch: 88, loss 130.14288330078125\n",
            "Episode finished\n",
            "Epoch: 89, loss 156.84739685058594\n",
            "Episode finished\n",
            "Epoch: 90, loss 110.19121551513672\n",
            "Episode finished\n",
            "Epoch: 91, loss 53.08824920654297\n",
            "Episode finished\n",
            "Epoch: 92, loss 77.58525085449219\n",
            "Episode finished\n",
            "Epoch: 93, loss 118.47903442382812\n",
            "Episode finished\n",
            "Epoch: 94, loss 123.42735290527344\n",
            "Episode finished\n",
            "Epoch: 95, loss 57.46137237548828\n",
            "Episode finished\n",
            "Epoch: 96, loss 76.60710144042969\n",
            "Episode finished\n",
            "Epoch: 97, loss 39.572853088378906\n",
            "Episode finished\n",
            "Epoch: 98, loss 26.71580696105957\n",
            "Episode finished\n",
            "Epoch: 99, loss 59.15766906738281\n",
            "Episode finished\n",
            "Epoch: 100, loss 23.87921714782715\n",
            "Episode finished\n",
            "Epoch: 101, loss 81.91154479980469\n",
            "Episode finished\n",
            "Epoch: 102, loss 101.24845123291016\n",
            "Episode finished\n",
            "Epoch: 103, loss 51.90105438232422\n",
            "Episode finished\n",
            "Epoch: 104, loss 175.84884643554688\n",
            "Episode finished\n",
            "Epoch: 105, loss 108.21803283691406\n",
            "Episode finished\n",
            "Epoch: 106, loss 35.24918746948242\n",
            "Episode finished\n",
            "Epoch: 107, loss 86.83028411865234\n",
            "Episode finished\n",
            "Epoch: 108, loss 104.66702270507812\n",
            "Episode finished\n",
            "Epoch: 109, loss 128.7642822265625\n",
            "Episode finished\n",
            "Epoch: 110, loss 115.31909942626953\n",
            "Episode finished\n",
            "Epoch: 111, loss 90.40431213378906\n",
            "Episode finished\n",
            "Epoch: 112, loss 44.31235122680664\n",
            "Episode finished\n",
            "Epoch: 113, loss 51.43571090698242\n",
            "Episode finished\n",
            "Epoch: 114, loss 172.19056701660156\n",
            "Episode finished\n",
            "Epoch: 115, loss 9.227762222290039\n",
            "Episode finished\n",
            "Epoch: 116, loss 58.30965042114258\n",
            "Episode finished\n",
            "Epoch: 117, loss 45.20948791503906\n",
            "Episode finished\n",
            "Epoch: 118, loss 90.01168823242188\n",
            "Episode finished\n",
            "Epoch: 119, loss 91.5464096069336\n",
            "Episode finished\n",
            "Epoch: 120, loss 43.62155532836914\n",
            "Episode finished\n",
            "Epoch: 121, loss 76.28380584716797\n",
            "Episode finished\n",
            "Epoch: 122, loss 54.03377151489258\n",
            "Episode finished\n",
            "Epoch: 123, loss 32.736568450927734\n",
            "Episode finished\n",
            "Epoch: 124, loss 19.131132125854492\n",
            "Episode finished\n",
            "Epoch: 125, loss 57.333194732666016\n",
            "Episode finished\n",
            "Epoch: 126, loss 87.9531478881836\n",
            "Episode finished\n",
            "Epoch: 127, loss 172.46328735351562\n",
            "Episode finished\n",
            "Epoch: 128, loss 123.29302215576172\n",
            "Episode finished\n",
            "Epoch: 129, loss 185.43678283691406\n",
            "Episode finished\n",
            "Epoch: 130, loss 180.4752960205078\n",
            "Episode finished\n",
            "Epoch: 131, loss 48.62592697143555\n",
            "Episode finished\n",
            "Epoch: 132, loss 49.017250061035156\n",
            "Episode finished\n",
            "Epoch: 133, loss 103.07302856445312\n",
            "Episode finished\n",
            "Epoch: 134, loss 201.06317138671875\n",
            "Episode finished\n",
            "Epoch: 135, loss 155.92434692382812\n",
            "Episode finished\n",
            "Epoch: 136, loss 201.61325073242188\n",
            "Episode finished\n",
            "Epoch: 137, loss 126.39155578613281\n",
            "Episode finished\n",
            "Epoch: 138, loss 120.59503936767578\n",
            "Episode finished\n",
            "Epoch: 139, loss 67.69847106933594\n",
            "Episode finished\n",
            "Epoch: 140, loss 29.059358596801758\n",
            "Episode finished\n",
            "Epoch: 141, loss 32.58583068847656\n",
            "Episode finished\n",
            "Epoch: 142, loss 2.8129053115844727\n",
            "Episode finished\n",
            "Epoch: 143, loss 18.33989143371582\n",
            "Episode finished\n",
            "Epoch: 144, loss 39.584957122802734\n",
            "Episode finished\n",
            "Epoch: 145, loss 61.124202728271484\n",
            "Episode finished\n",
            "Epoch: 146, loss 3.819718837738037\n",
            "Episode finished\n",
            "Epoch: 147, loss 29.121089935302734\n",
            "Episode finished\n",
            "Epoch: 148, loss 84.44518280029297\n",
            "Episode finished\n",
            "Epoch: 149, loss 48.61402893066406\n",
            "Episode finished\n",
            "Epoch: 150, loss 81.03628540039062\n",
            "Episode finished\n",
            "Epoch: 151, loss 65.98590850830078\n",
            "Episode finished\n",
            "Epoch: 152, loss 39.55633544921875\n",
            "Episode finished\n",
            "Epoch: 153, loss 16.643081665039062\n",
            "Episode finished\n",
            "Epoch: 154, loss 26.22269058227539\n",
            "Episode finished\n",
            "Epoch: 155, loss 106.87062072753906\n",
            "Episode finished\n",
            "Epoch: 156, loss 93.40654754638672\n",
            "Episode finished\n",
            "Epoch: 157, loss 71.3237075805664\n",
            "Episode finished\n",
            "Epoch: 158, loss 152.024658203125\n",
            "Episode finished\n",
            "Epoch: 159, loss 97.54109191894531\n",
            "Episode finished\n",
            "Epoch: 160, loss 92.79237365722656\n",
            "Episode finished\n",
            "Epoch: 161, loss 129.97886657714844\n",
            "Episode finished\n",
            "Epoch: 162, loss 150.76356506347656\n",
            "Episode finished\n",
            "Epoch: 163, loss 173.12930297851562\n",
            "Episode finished\n",
            "Epoch: 164, loss 105.97544860839844\n",
            "Episode finished\n",
            "Epoch: 165, loss 120.02511596679688\n",
            "Episode finished\n",
            "Epoch: 166, loss 142.64447021484375\n",
            "Episode finished\n",
            "Epoch: 167, loss 120.73360443115234\n",
            "Episode finished\n",
            "Epoch: 168, loss 99.10137939453125\n",
            "Episode finished\n",
            "Epoch: 169, loss 141.7716064453125\n",
            "Episode finished\n",
            "Epoch: 170, loss 57.119834899902344\n",
            "Episode finished\n",
            "Epoch: 171, loss 106.69098663330078\n",
            "Episode finished\n",
            "Epoch: 172, loss 84.97551727294922\n",
            "Episode finished\n",
            "Epoch: 173, loss 38.59652328491211\n",
            "Episode finished\n",
            "Epoch: 174, loss 101.223876953125\n",
            "Episode finished\n",
            "Epoch: 175, loss 25.634843826293945\n",
            "Episode finished\n",
            "Epoch: 176, loss 80.62454223632812\n",
            "Episode finished\n",
            "Epoch: 177, loss 45.8848991394043\n",
            "Episode finished\n",
            "Epoch: 178, loss 79.76712799072266\n",
            "Episode finished\n",
            "Epoch: 179, loss 62.66515350341797\n",
            "Episode finished\n",
            "Epoch: 180, loss 124.5936050415039\n",
            "Episode finished\n",
            "Epoch: 181, loss 96.89859008789062\n",
            "Episode finished\n",
            "Epoch: 182, loss 75.2303466796875\n",
            "Episode finished\n",
            "Epoch: 183, loss 44.70091247558594\n",
            "Episode finished\n",
            "Epoch: 184, loss 37.20184326171875\n",
            "Episode finished\n",
            "Epoch: 185, loss 46.69818115234375\n",
            "Episode finished\n",
            "Epoch: 186, loss 54.723365783691406\n",
            "Episode finished\n",
            "Epoch: 187, loss 6.12753963470459\n",
            "Episode finished\n",
            "Epoch: 188, loss 42.473846435546875\n",
            "Episode finished\n",
            "Epoch: 189, loss 58.444759368896484\n",
            "Episode finished\n",
            "Epoch: 190, loss 40.97237777709961\n",
            "Episode finished\n",
            "Epoch: 191, loss 69.67665100097656\n",
            "Episode finished\n",
            "Epoch: 192, loss 86.4849853515625\n",
            "Episode finished\n",
            "Epoch: 193, loss 31.11127471923828\n",
            "Episode finished\n",
            "Epoch: 194, loss 84.0352554321289\n",
            "Episode finished\n",
            "Epoch: 195, loss 16.438884735107422\n",
            "Episode finished\n",
            "Epoch: 196, loss 80.43406677246094\n",
            "Episode finished\n",
            "Epoch: 197, loss 89.26879119873047\n",
            "Episode finished\n",
            "Epoch: 198, loss 155.1047821044922\n",
            "Episode finished\n",
            "Epoch: 199, loss 157.36842346191406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nLqWqCWcAO0c",
        "colab_type": "code",
        "outputId": "163e152c-f318-4381-87de-7763b5972749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "s_t = agent.env.reset()\n",
        "plt.figure(figsize=(9,9))\n",
        "\n",
        "for _ in range(2000):\n",
        "    plt.imshow(agent.env.render(mode='rgb_array')) # just update the data\n",
        "    #display.display(plt.gcf())    \n",
        "    display.clear_output(wait=True)\n",
        "    agent.get_action(torch.unsqueeze(agent.pix2tensor(s_t),0))\n",
        "    s_t, r, done, info = agent.env.step(agent.a_t_l[-1])\n",
        "agent.env.close()"
      ],
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAILCAYAAAA+FjEmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 648x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}